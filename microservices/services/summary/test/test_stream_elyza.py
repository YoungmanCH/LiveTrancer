# -*- coding: utf-8 -*-
"""test_stream_elyza.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18Stpu3wz5jntic2IKfVyzk_LpvveAnbd
"""

!pip install transformers accelerate bitsandbytes

from transformers import AutoModelForCausalLM, AutoTokenizer
import torch
from transformers import TextIteratorStreamer
from threading import Thread
import time

tokenizer = AutoTokenizer.from_pretrained(
    "elyza/ELYZA-japanese-Llama-2-7b-instruct"
)
model = AutoModelForCausalLM.from_pretrained(
    "elyza/ELYZA-japanese-Llama-2-7b-instruct",
    torch_dtype=torch.float16,
    device_map="auto"
)

prompt = """「Buildxを使うことで、異なるアーキテクチャ（例：linux/arm64 や linux/ppc64le など）に向けたイメージもビルド可能で、クロスプラットフォームのビルドに特化しています。」という文章を、中学生にも分かりやすいよう言い換えてください。"""

st = time.time()
with torch.no_grad():
    streamer = TextIteratorStreamer(tokenizer)
    token_ids = tokenizer.encode(prompt, add_special_tokens=False, return_tensors="pt")
    generation_kwargs = dict(
        input_ids=token_ids.to(model.device),
        max_new_tokens=256,
        pad_token_id=tokenizer.pad_token_id,
        eos_token_id=tokenizer.eos_token_id,
        streamer=streamer,
    )
    thread = Thread(target=model.generate, kwargs=generation_kwargs)
    thread.start()
fin = time.time()
print(f"time is :{fin - st:.3f}", end = "")
# 出力
for new_text in streamer:
    if "「" in new_text :
      f = time.time()
      print(f"<<time is :{f - st:.3f}s>>",end = "")
    print(new_text.replace(" ", ""), end="")

